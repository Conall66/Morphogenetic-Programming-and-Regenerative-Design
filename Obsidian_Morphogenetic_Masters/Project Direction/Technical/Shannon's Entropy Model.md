Quantifies the uncertainty or randomness in a source of information, providing a measure of the information content or "entropy" associated with a probability distribution

$H(X)=−i=1∑n​P(x_i​)log_b​P(x_i​)$

- Where $H(X)$ is the average uncertainty or "entropy" of the random variable X
- $P(X)$ is the average uncertainty of variable X

For example, with a weighted coin, the entropy value (measures in bits) is lower than 1 (even probability) because the outcome is more predictable


